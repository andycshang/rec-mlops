# æ–‡ä»¶ä½ç½®: src/init_delta_tables.py
import os
import time
from pyspark.sql import SparkSession
from pyspark.sql.types import StructType, StructField, LongType, DoubleType, StringType
from delta import configure_spark_with_delta_pip

def init_delta_tables():
    print("ğŸš€ Starting Delta Lake initialization inside Docker...")
    
    # 1. é…ç½® Spark + Delta
    builder = SparkSession.builder.appName("DeltaSetup") \
        .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension") \
        .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog")
    
    spark = configure_spark_with_delta_pip(builder).getOrCreate()
    
    # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬ä½¿ç”¨ /tmp/delta-tablesï¼Œä¸ setup.py ä¿æŒä¸€è‡´
    # ä½†å»ºè®®ç”Ÿäº§ç¯å¢ƒæ”¹ä¸º /data/delta-tables ä»¥ä¾¿æŒä¹…åŒ–
    delta_path = "/tmp/delta-tables"
    os.makedirs(delta_path, exist_ok=True)
    
    # 2. åˆ›å»º Interactions è¡¨
    print("ğŸ“¦ Creating interactions table...")
    interactions_schema = StructType([
        StructField("user_id", LongType(), True),
        StructField("item_id", LongType(), True),
        StructField("rating", DoubleType(), True),
        StructField("interaction_type", StringType(), True),
        StructField("timestamp", DoubleType(), True),
        StructField("session_id", StringType(), True)
    ])
    
    # ç”Ÿæˆä¸€äº›æ ·æœ¬æ•°æ®
    sample_data = []
    for i in range(1000):
        sample_data.append((
            int(i % 20),           # user_id
            int(i % 50),            # item_id
            float(3.0 + (i % 2)),  # rating
            "rating",              # interaction_type
            float(time.time()),    # timestamp
            f"session_{i}"         # session_id
        ))
    
    df = spark.createDataFrame(sample_data, interactions_schema)
    
    # å†™å…¥ Delta Lake
    df.write.format("delta").mode("overwrite").save(f"{delta_path}/interactions")
    print(f"âœ… Interactions table created at {delta_path}/interactions")
    
    # 3. åˆ›å»º User Profiles è¡¨
    print("ğŸ‘¤ Creating user_profiles table...")
    user_schema = StructType([
        StructField("user_id", LongType(), True),
        StructField("avg_rating", DoubleType(), True),
        StructField("interaction_count", LongType(), True),
        StructField("last_interaction", DoubleType(), True)
    ])
    
    # åˆ›å»ºç©ºè¡¨æˆ–æ ·æœ¬æ•°æ®
    user_data = [(1, 4.5, 10, float(time.time()))]
    user_df = spark.createDataFrame(user_data, user_schema)
    
    user_df.write.format("delta").mode("overwrite").save(f"{delta_path}/user_profiles")
    print(f"âœ… User profiles table created at {delta_path}/user_profiles")
    
    spark.stop()
    print("ğŸ‰ Initialization complete!")

if __name__ == "__main__":
    init_delta_tables()